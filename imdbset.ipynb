{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDb dataset from Hugging Face\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Convert train and test datasets to Pandas DataFrames\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Check the DataFrame\n",
    "print(train_df.sample(5))\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense,Activation\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the text data using Keras Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)  # Limit the number of words (vocabulary size) to 5000\n",
    "tokenizer.fit_on_texts(train_df['text'])  # Fit tokenizer on training data\n",
    "\n",
    "# Convert the text to sequences of integers\n",
    "X_train = tokenizer.texts_to_sequences(train_df['text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "# Pad the sequences to ensure they are all the same length\n",
    "max_sequence_length = 130  # Define max sequence length (for padding)\n",
    "X_train = pad_sequences(X_train, maxlen=max_sequence_length, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Prepare the labels (sentiment labels: 0 or 1)\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6810692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocab_size and embedding_dim\n",
    "vocab_size = 5000\n",
    "embedding_dim = 100\n",
    "\n",
    "# Build the SimpleRNN model with an Embedding layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=len(X_train[0]))) # Embedding layer\n",
    "model.add(SimpleRNN(32,return_sequences=False, activation=\"relu\"))  # SimpleRNN layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer (binary classification)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fdb0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42debb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 3,batch_size=128,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84faf098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"],label=\"Train\");\n",
    "plt.plot(history.history[\"val_accuracy\"],label=\"Test\");\n",
    "plt.title(\"Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"],label=\"Train\");\n",
    "plt.plot(history.history[\"val_loss\"],label=\"Test\");\n",
    "plt.title(\"Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b5a3d",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd24bd5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Check distribution of text lengths (in terms of number of words)\n",
    "train_df['text_length'] = train_df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Plot the distribution\n",
    "train_df['text_length'].hist(bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Text Length Distribution in IMDb Dataset')\n",
    "plt.xlabel('Number of Words in Review')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Check quantiles to choose a reasonable max_sequence_length\n",
    "print(f\"Quantiles of text length: \\n{train_df['text_length'].quantile([0.25, 0.5, 0.75, 0.9, 0.95, 1.0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7272dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check text length distribution for both train and test datasets\n",
    "train_df['text_length'] = train_df['text'].apply(len)\n",
    "test_df['text_length'] = test_df['text'].apply(len)\n",
    "\n",
    "# Plot the distribution of text lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_df['text_length'], bins=50, kde=True, color='blue', label='Train Set')\n",
    "sns.histplot(test_df['text_length'], bins=50, kde=True, color='orange', label='Test Set')\n",
    "plt.title('Distribution of Text Lengths')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Show a few examples of reviews from the dataset\n",
    "print(train_df['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae9dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution for both train and test datasets\n",
    "train_class_distribution = train_df['label'].value_counts(normalize=True)\n",
    "test_class_distribution = test_df['label'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Train class distribution:\", train_class_distribution)\n",
    "print(\"Test class distribution:\", test_class_distribution)\n",
    "\n",
    "# Plot the class distribution\n",
    "train_class_distribution.plot(kind='bar', color=['blue', 'red'], alpha=0.7)\n",
    "plt.title(\"Train Data Class Distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'], rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9729e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Combine all reviews into one text for word cloud\n",
    "all_reviews = ' '.join(train_df['text'].values)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_reviews)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ec86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot of review length vs sentiment\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='label', y='text_length', data=train_df)\n",
    "plt.title(\"Review Length vs Sentiment\")\n",
    "plt.xlabel(\"Sentiment (0=Negative, 1=Positive)\")\n",
    "plt.ylabel(\"Review Length (number of words)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a84b2de",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7346934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "# Initialize W&B project\n",
    "wandb.init(project=\"imdb_sentiment_analysis_Experimentation_for conclusion\", name=\"rnn_model_training_Adam_32_001\")\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 0.001  # Different learning rates\n",
    "batch_size = 32  # Different batch sizes\n",
    "epochs = 3\n",
    "\n",
    "# Set the optimizer with the current learning rate\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "# Initialize the W&B run for this set of hyperparameters\n",
    "wandb.init(project=\"imdb_sentiment_analysis\", name=f\"rnn_lr_{lr}_batch_{batch_size}\")\n",
    "\n",
    "# Train the model manually and log the metrics after each epoch\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} with Learning Rate {lr} and Batch Size {batch_size}\")\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "    # Get the logs for the current epoch\n",
    "    logs = history.history\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,  # Current epoch number\n",
    "        \"training_loss\": logs.get(\"loss\")[0],  # Training loss for the current epoch\n",
    "        \"validation_loss\": logs.get(\"val_loss\")[0],  # Validation loss for the current epoch\n",
    "        \"validation_accuracy\": logs.get(\"val_accuracy\")[0],  # Validation accuracy for the current epoch\n",
    "        \"learning_rate\": model.optimizer.learning_rate.numpy(),  # Learning rate\n",
    "    })\n",
    "\n",
    "# After training, evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the output\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Log the results to W&B\n",
    "wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n",
    "\n",
    "# Log model summary to W&B\n",
    "wandb.log({\"model_summary\": model.summary()})\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e62422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "# Initialize W&B project\n",
    "wandb.init(project=\"imdb_sentiment_analysis_Experimentation_for conclusion\", name=\"rnn_model_training_Adam_64_001\")\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 0.001  # Different learning rates\n",
    "batch_size = 64  # Different batch sizes\n",
    "epochs = 3\n",
    "\n",
    "# Set the optimizer with the current learning rate\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "# Initialize the W&B run for this set of hyperparameters\n",
    "wandb.init(project=\"imdb_sentiment_analysis\", name=f\"rnn_lr_{lr}_batch_{batch_size}\")\n",
    "\n",
    "# Train the model manually and log the metrics after each epoch\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} with Learning Rate {lr} and Batch Size {batch_size}\")\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "    # Get the logs for the current epoch\n",
    "    logs = history.history\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,  # Current epoch number\n",
    "        \"training_loss\": logs.get(\"loss\")[0],  # Training loss for the current epoch\n",
    "        \"validation_loss\": logs.get(\"val_loss\")[0],  # Validation loss for the current epoch\n",
    "        \"validation_accuracy\": logs.get(\"val_accuracy\")[0],  # Validation accuracy for the current epoch\n",
    "        \"learning_rate\": model.optimizer.learning_rate.numpy(),  # Learning rate\n",
    "    })\n",
    "\n",
    "# After training, evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the output\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Log the results to W&B\n",
    "wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n",
    "\n",
    "# Log model summary to W&B\n",
    "wandb.log({\"model_summary\": model.summary()})\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf66e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "# Initialize W&B project\n",
    "wandb.init(project=\"imdb_sentiment_analysis_Experimentation_for conclusion\", name=\"rnn_model_training_Adam_64_0005\")\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 0.0005  # Different learning rates\n",
    "batch_size = 64  # Different batch sizes\n",
    "epochs = 3\n",
    "\n",
    "# Set the optimizer with the current learning rate\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "# Initialize the W&B run for this set of hyperparameters\n",
    "wandb.init(project=\"imdb_sentiment_analysis\", name=f\"rnn_lr_{lr}_batch_{batch_size}\")\n",
    "\n",
    "# Train the model manually and log the metrics after each epoch\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} with Learning Rate {lr} and Batch Size {batch_size}\")\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "    # Get the logs for the current epoch\n",
    "    logs = history.history\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,  # Current epoch number\n",
    "        \"training_loss\": logs.get(\"loss\")[0],  # Training loss for the current epoch\n",
    "        \"validation_loss\": logs.get(\"val_loss\")[0],  # Validation loss for the current epoch\n",
    "        \"validation_accuracy\": logs.get(\"val_accuracy\")[0],  # Validation accuracy for the current epoch\n",
    "        \"learning_rate\": model.optimizer.learning_rate.numpy(),  # Learning rate\n",
    "    })\n",
    "\n",
    "# After training, evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the output\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Log the results to W&B\n",
    "wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n",
    "\n",
    "# Log model summary to W&B\n",
    "wandb.log({\"model_summary\": model.summary()})\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "# Initialize W&B project\n",
    "wandb.init(project=\"imdb_sentiment_analysis_Experimentation_for conclusion\", name=\"rnn_model_training_SGD_64_0005\")\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 0.0005  # Different learning rates\n",
    "batch_size = 64  # Different batch sizes\n",
    "epochs = 3\n",
    "\n",
    "# Set the optimizer with the current learning rate\n",
    "optimizer = SGD(learning_rate=lr)\n",
    "\n",
    "# Initialize the W&B run for this set of hyperparameters\n",
    "wandb.init(project=\"imdb_sentiment_analysis\", name=f\"rnn_lr_{lr}_batch_{batch_size}\")\n",
    "\n",
    "# Train the model manually and log the metrics after each epoch\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} with Learning Rate {lr} and Batch Size {batch_size}\")\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "    # Get the logs for the current epoch\n",
    "    logs = history.history\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,  # Current epoch number\n",
    "        \"training_loss\": logs.get(\"loss\")[0],  # Training loss for the current epoch\n",
    "        \"validation_loss\": logs.get(\"val_loss\")[0],  # Validation loss for the current epoch\n",
    "        \"validation_accuracy\": logs.get(\"val_accuracy\")[0],  # Validation accuracy for the current epoch\n",
    "        \"learning_rate\": model.optimizer.learning_rate.numpy(),  # Learning rate\n",
    "    })\n",
    "\n",
    "# After training, evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the output\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Log the results to W&B\n",
    "wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n",
    "\n",
    "# Log model summary to W&B\n",
    "wandb.log({\"model_summary\": model.summary()})\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "# Initialize W&B project\n",
    "wandb.init(project=\"imdb_sentiment_analysis_Experimentation_for conclusion\", name=\"rnn_model_training_RMS_64_0005\")\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 0.0005  # Different learning rates\n",
    "batch_size = 64  # Different batch sizes\n",
    "epochs = 3\n",
    "\n",
    "# Set the optimizer with the current learning rate\n",
    "optimizer = RMSprop(learning_rate=lr)\n",
    "\n",
    "# Initialize the W&B run for this set of hyperparameters\n",
    "wandb.init(project=\"imdb_sentiment_analysis\", name=f\"rnn_lr_{lr}_batch_{batch_size}\")\n",
    "\n",
    "# Train the model manually and log the metrics after each epoch\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} with Learning Rate {lr} and Batch Size {batch_size}\")\n",
    "\n",
    "    # Train the model for one epoch\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "    # Get the logs for the current epoch\n",
    "    logs = history.history\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,  # Current epoch number\n",
    "        \"training_loss\": logs.get(\"loss\")[0],  # Training loss for the current epoch\n",
    "        \"validation_loss\": logs.get(\"val_loss\")[0],  # Validation loss for the current epoch\n",
    "        \"validation_accuracy\": logs.get(\"val_accuracy\")[0],  # Validation accuracy for the current epoch\n",
    "        \"learning_rate\": model.optimizer.learning_rate.numpy(),  # Learning rate\n",
    "    })\n",
    "\n",
    "# After training, evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the output\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Log the results to W&B\n",
    "wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n",
    "\n",
    "# Log model summary to W&B\n",
    "wandb.log({\"model_summary\": model.summary()})\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
